{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.7.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m290.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/saumickpradhan/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m323.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.14.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Downloading openai-1.7.2-py3-none-any.whl (212 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 kB\u001b[0m \u001b[31m380.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m691.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m430.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m305.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.6-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m243.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m416.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m218.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, sniffio, pydantic-core, idna, h11, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.2.0 certifi-2023.11.17 distro-1.9.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 idna-3.6 openai-1.7.2 pydantic-2.5.3 pydantic-core-2.14.6 sniffio-1.3.0 tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      4\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-e31CdGNZra62pEFq31H6T3BlbkFJqi4xRZSPzpfrsAPEVNQb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a coding assignment creator, assignment grader, feedback provider and coding tutorial platform.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello ChatGPT, does this work?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"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\")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a coding assignment creator, assignment grader, feedback provider and coding tutorial platform.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello ChatGPT, does this work?\"}\n",
    "  ],\n",
    "  temperature=0.5,\n",
    "  max_tokens=10\n",
    "  )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Coding Tutorial Chatbot!\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, code_solution)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m code_solution \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, code_solution)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mgenerate_code\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_code\u001b[39m(query):\n\u001b[1;32m      6\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode a solution for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'Ysk-e31CdGNZra62pEFq31H6T3BlbkFJqi4xRZSPzpfrsAPEVNQb'\n",
    "\n",
    "def generate_code(query):\n",
    "    prompt = f\"Code a solution for: {query}\\n\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=4\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to Coding Tutorial Chatbot!\")\n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        code_solution = generate_code(user_query)\n",
    "        print(\"Chatbot:\", code_solution)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exercise():\n",
    "    # Prompt for exercise generation\n",
    "    prompt = \"Generate a unique Python coding exercise related to \"\n",
    "\n",
    "    # Request completion from OpenAI GPT-3.5 Turbo\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=200,  # Adjust as needed\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # Extract and return the generated exercise\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saumickpradhan/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages (from requests) (3.6)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saumickpradhan/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages (from requests) (2023.11.17)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.9/120.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, charset-normalizer, requests\n",
      "Successfully installed charset-normalizer-3.3.2 requests-2.31.0 urllib3-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to OpenAI API...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 92\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# with open('embeddings.json', 'r') as f:\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#     embeddings_dict = json.load(f)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# print(f\"Annoy index loaded: {annoy_index.get_n_items()} items\")\u001b[39;00m\n\u001b[1;32m     89\u001b[0m conversation_history \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a coding Teaching Assistant to teach python programming to first year college students. You will review student submissions and provide feedback to their response without givng just a hint.\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m---> 92\u001b[0m \u001b[43minteract_with_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHere is my code: pRint(dsds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_history\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m, in \u001b[0;36minteract_with_gpt\u001b[0;34m(user_input, conversation_history)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request to OpenAI API...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# app.logger.info(\"Sending request to OpenAI API...\")\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openai.com/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39mdata, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived response from OpenAI API\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# app.logger.info(\"Received response from OpenAI\")\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def interact_with_gpt(user_input, conversation_history):\n",
    "    context = \"\"\n",
    "    # most_similar_files = [] \n",
    "    # if embeddings_lookup_enabled:\n",
    "    #     app.logger.info(f\"Searching in embeddings for user input: {user_input}\")\n",
    "    #     processed_user_input = preprocess_code(user_input, 'py')\n",
    "    #     user_input_embeddings = generate_embeddings_for_code(processed_user_input)\n",
    "    #     most_similar_files = find_most_similar_files(annoy_index, index_map, user_input_embeddings)\n",
    "\n",
    "    #     for file_path in most_similar_files:\n",
    "    #         with open(file_path, 'r') as f:\n",
    "    #             code_snippet = f.read()[:500]  # Limit the length of the code snippet\n",
    "    #             context += f\"\\n\\n--- {file_path} ---\\n{code_snippet}\\n\"\n",
    "    #     relevant_code_files = ', '.join([f\"{i+1}. {file}\" for i, file in enumerate(most_similar_files)])\n",
    "    #     conversation_history.append({\"role\": \"user\", \"content\": f\"Relevant code files: {relevant_code_files}\"})\n",
    "    # else:\n",
    "    #     app.logger.info(f\"Embeddings lookup disabled for user input: {user_input}\")\n",
    "\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": context})\n",
    "\n",
    "    # token_usage = calculate_token_count(conversation_history)\n",
    "    # token_limit = 8000\n",
    "\n",
    "    # if token_usage >= token_limit:\n",
    "    #     app.logger.warning(\"Token limit reached. Clearing conversation history and asking user to rephrase the query.\")\n",
    "    #     conversation_history.clear()\n",
    "    #     conversation_history.append({\"role\": \"system\", \"content\": \"You are a GPT-4 coding assistant helping with code.\"})\n",
    "    #     assistant_response = \"Warning: Conversation history cleared due to reaching the token limit. Please rephrase your query.\"\n",
    "    #     conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    #     return {\"response\": assistant_response, \"token_usage\": 0}\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": conversation_history,\n",
    "        \"max_tokens\": 120,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai.api_key}\"\n",
    "    }\n",
    "\n",
    "    print(\"Sending request to OpenAI API...\")\n",
    "    # app.logger.info(\"Sending request to OpenAI API...\")\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", json=data, headers=headers)\n",
    "    print(\"Received response from OpenAI API\")\n",
    "    # app.logger.info(\"Received response from OpenAI\")\n",
    "                    \n",
    "    response_data = response.json()\n",
    "\n",
    "    print(\"Conversation history:\", conversation_history)\n",
    "    print(\"Response data:\", response_data)\n",
    "\n",
    "    if response_data.get(\"choices\") and response_data[\"choices\"][0].get(\"message\"):\n",
    "        assistant_response = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        # # Add the content of the most similar files to the response dictionary\n",
    "        # most_similar_file_contents = []\n",
    "        # for file_path in most_similar_files:\n",
    "        #     with open(file_path, 'r') as f:\n",
    "        #         file_content = f.read()\n",
    "        #         most_similar_file_contents.append(file_content)\n",
    "\n",
    "        #         print(\"Assistant response:\", assistant_response)  # Debug print\n",
    "        #         result = {\"response\": assistant_response, \"token_usage\": token_usage, \"most_similar_files\": most_similar_files, \"most_similar_file_contents\": most_similar_file_contents}\n",
    "        #         return result\n",
    "\n",
    "        result = {\"response\": assistant_response}#, \"token_usage\": token_usage, \"most_similar_files\": most_similar_files, \"most_similar_file_contents\": most_similar_file_contents}\n",
    "        return result     \n",
    "\n",
    "\n",
    "\n",
    "# with open('embeddings.json', 'r') as f:\n",
    "#     embeddings_dict = json.load(f)\n",
    "\n",
    "# with open('index_map.json', 'r') as f:\n",
    "#     index_map = json.load(f)\n",
    "\n",
    "# annoy_index = AnnoyIndex(768, 'angular')  # 768 is the embedding dimension\n",
    "# annoy_index.load('embeddings.ann')\n",
    "\n",
    "# print(f\"Annoy index loaded: {annoy_index.get_n_items()} items\")\n",
    "\n",
    "conversation_history = [{\"role\": \"system\", \"content\": \"You are a coding Teaching Assistant to teach python programming to first year college students. You will review student submissions and provide feedback to their response without givng just a hint.\"}]\n",
    "\n",
    "\n",
    "interact_with_gpt(\"Here is my code: pRint(dsds')\", conversation_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     72\u001b[0m topic_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a Python function that calculates the factorial of a given number.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m solutions \u001b[38;5;241m=\u001b[39m generate_solutions(questions)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Questions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mgenerate_assignment\u001b[0;34m(topic_description)\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate three Python coding questions based on the following topic description:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtopic_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate questions\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m question1 \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate second question\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize OpenAI API with your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_assignment(topic_description):\n",
    "    prompt = f\"Create three Python coding questions based on the following topic description:\\n\\n{topic_description}\\n\\n1)\"\n",
    "    \n",
    "    # Generate questions\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"2)\"]\n",
    "    )\n",
    "    \n",
    "    question1 = response.choices[0].text.strip()\n",
    "    \n",
    "    # Generate second question\n",
    "    prompt += f\"\\n\\n2)\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"3)\"]\n",
    "    )\n",
    "    question2 = response.choices[0].text.strip()\n",
    "    \n",
    "    # Generate third question\n",
    "    prompt += f\"\\n\\n3)\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"4)\"]\n",
    "    )\n",
    "    question3 = response.choices[0].text.strip()\n",
    "    \n",
    "    return [question1, question2, question3]\n",
    "\n",
    "def generate_solutions(questions):\n",
    "    solutions = []\n",
    "    for question in questions:\n",
    "        # Generate solutions for each question\n",
    "        prompt = f\"Write a solution for the following Python coding question:\\n\\n{question}\\n\\nSolution:\"\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"gpt-3.5-turbo\",\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=100,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=[\"\\n\\n\"]\n",
    "        )\n",
    "        solutions.append(response.choices[0].text.strip())\n",
    "    return solutions\n",
    "\n",
    "# Example usage\n",
    "topic_description = \"Write a Python function that calculates the factorial of a given number.\"\n",
    "questions = generate_assignment(topic_description)\n",
    "solutions = generate_solutions(questions)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "\n",
    "print(\"\\nGenerated Solutions:\")\n",
    "for i, solution in enumerate(solutions):\n",
    "    print(f\"Solution {i+1}: {solution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'chatCompletion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     72\u001b[0m topic_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a Python function that calculates the factorial of a given number.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m solutions \u001b[38;5;241m=\u001b[39m generate_solutions(questions)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Questions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mgenerate_assignment\u001b[0;34m(topic_description)\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate three Python coding questions based on the following topic description:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtopic_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate questions\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchatCompletion\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     11\u001b[0m     engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     13\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     14\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     15\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     16\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     17\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     18\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2)\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m question1 \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate second question\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'chatCompletion'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize OpenAI API with your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_assignment(topic_description):\n",
    "    prompt = f\"Create three Python coding questions based on the following topic description:\\n\\n{topic_description}\\n\\n1)\"\n",
    "\n",
    "    # Generate questions\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"2)\"]\n",
    "    )\n",
    "\n",
    "    question1 = response.choices[0].text.strip()\n",
    "\n",
    "    # Generate second question\n",
    "    prompt += f\"\\n\\n2)\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"3)\"]\n",
    "    )\n",
    "    question2 = response.choices[0].text.strip()\n",
    "\n",
    "    # Generate third question\n",
    "    prompt += f\"\\n\\n3)\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"4)\"]\n",
    "    )\n",
    "    question3 = response.choices[0].text.strip()\n",
    "\n",
    "    return [question1, question2, question3]\n",
    "\n",
    "def generate_solutions(questions):\n",
    "    solutions = []\n",
    "    for question in questions:\n",
    "        # Generate solutions for each question\n",
    "        prompt = f\"Write a solution for the following Python coding question:\\n\\n{question}\\n\\nSolution:\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=100,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=[\"\\n\\n\"]\n",
    "        )\n",
    "        solutions.append(response.choices[0].text.strip())\n",
    "    return solutions\n",
    "\n",
    "# Example usage\n",
    "topic_description = \"Write a Python function that calculates the factorial of a given number.\"\n",
    "questions = generate_assignment(topic_description)\n",
    "solutions = generate_solutions(questions)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "\n",
    "print(\"\\nGenerated Solutions:\")\n",
    "for i, solution in enumerate(solutions):\n",
    "    print(f\"Solution {i+1}: {solution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested loops\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m solutions \u001b[38;5;241m=\u001b[39m generate_solutions(topic)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Questions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mgenerate_questions\u001b[0;34m(topic)\u001b[0m\n\u001b[1;32m      7\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs a Teaching Assistant for CS 101, I need to generate three well-formatted Python coding questions based on the topic \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m query(payload)\n\u001b[0;32m---> 11\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerated_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m questions\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def generate_questions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"As a Teaching Assistant for CS 101, I need to generate three well-formatted Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    questions = response[\"generated_text\"].split(\"\\n\\n\")\n",
    "    return questions\n",
    "\n",
    "def generate_solutions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"Provide solutions for Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    solutions = response[\"generated_text\"]\n",
    "    return solutions\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage:\n",
    "topic = \"nested loops\"\n",
    "questions = generate_questions(topic)\n",
    "solutions = generate_solutions(topic)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "\n",
    "print(\"\\nSolutions:\")\n",
    "print(solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "Question 1: You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'nested loops'.\n",
      "Question 2: Topic: Nested Loops\n",
      "Question 3: Question:\n",
      "Question 4: Write a Python program that uses nested loops to calculate the sum of the squares of the first 10 positive integers.\n",
      "Question 5: Answer:\n",
      "Question 6: ```python\n",
      "# Initialize the sum of squares\n",
      "sum_of_squares = 0\n",
      "Question 7: # Outer loop for the first 10 positive integers\n",
      "for i in range(1, 11):\n",
      "    # In\n",
      "\n",
      "Solutions:\n",
      "Provide solutions for Python coding questions based on the topic 'nested loops'.\n",
      "\n",
      "1. Write a Python program that uses nested loops to print the multiplication table for numbers 1 through 10.\n",
      "\n",
      "```python\n",
      "for i in range(1, 11):\n",
      "    for j in range(1, 11):\n",
      "        print(f\"{i} x {j} = {i*j}\")\n",
      "    print()\n",
      "```\n",
      "\n",
      "2. Write a Python program that uses nested loops to find the sum of\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "\n",
    "def generate_questions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    generated_text = response[0][\"generated_text\"]\n",
    "    return generated_text.split(\"\\n\\n\")\n",
    "\n",
    "def generate_solutions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"Provide solutions for Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    solutions = response[0][\"generated_text\"]\n",
    "    return solutions\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage:\n",
    "topic = \"nested loops\"\n",
    "questions = generate_questions(topic)\n",
    "solutions = generate_solutions(topic)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "\n",
    "print(\"\\nSolutions:\")\n",
    "print(solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'nested loops'.\n",
      "Topic: Nested Loops\n",
      "Question:\n",
      "Write a Python program that uses nested loops to calculate the sum of the squares of the first 10 positive integers.\n",
      "Answer:\n",
      "```python\n",
      "# Initialize the sum of squares\n",
      "sum_of_squares = 0\n",
      "# Outer loop for the first 10 positive integers\n",
      "for i in range(1, 11):\n",
      "    # In\n",
      "\n",
      "Solution:\n",
      "Provide solutions for Python coding questions based on the topic 'nested loops'.\n",
      "\n",
      "1. Write a Python program that uses nested loops to print the multiplication table for numbers 1 through 10.\n",
      "\n",
      "```python\n",
      "for i in range(1, 11):\n",
      "    for j in range(1, 11):\n",
      "        print(f\"{i} x {j} = {i*j}\")\n",
      "    print()\n",
      "```\n",
      "\n",
      "2. Write a Python program that uses nested loops to find the sum of\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "\n",
    "def generate_questions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    generated_text = response[0][\"generated_text\"]\n",
    "    return generated_text.split(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def generate_solutions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"Provide solutions for Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    solutions = response[0][\"generated_text\"]\n",
    "    return solutions\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    # Placeholder implementation\n",
    "    # This function should make a request to the API and return the response\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "topic = \"nested loops\"\n",
    "questions = generate_questions(topic)\n",
    "solutions = generate_solutions(topic)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(question)\n",
    "\n",
    "print(\"\\nSolution:\")\n",
    "print(solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question for students:\n",
      "Can you please let us know more details about your Write a function to calculate the factorial of a number.\n",
      "\n",
      "The factorial of a number is the product of all positive integers less than or equal to that number. For example, the factorial of 5 is 5*4*3*2*1 = 120.\n",
      "\n",
      "We need a function that takes an integer as input and returns the factorial of that number.\n",
      "\n",
      "Here is a Python code that calculates the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "Your solution is incorrect. Try again or ask for a hint.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def generate_question(description):\n",
    "    # Generate question based on the description using the OpenChat API\n",
    "    output = query({\n",
    "        \"inputs\": \"Can you please let us know more details about your \" + description,\n",
    "    })\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = output[0]['generated_text']\n",
    "    return generated_text\n",
    "\n",
    "def check_solution(student_solution, correct_solution):\n",
    "    # Check if student's solution matches the correct solution\n",
    "    if student_solution == correct_solution:\n",
    "        return True, \"Congratulations! Your solution is correct.\"\n",
    "    else:\n",
    "        # Provide hint or feedback based on the student's solution\n",
    "        # You can implement your own logic here based on the requirements\n",
    "        # For simplicity, let's assume providing a generic feedback\n",
    "        return False, \"Your solution is incorrect. Try again or ask for a hint.\"\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # 1. Professor inputs description\n",
    "    professor_description = \"Write a function to calculate the factorial of a number.\"\n",
    "    # 2. Platform generates question\n",
    "    question = generate_question(professor_description)\n",
    "    print(\"Question for students:\")\n",
    "    print(question)\n",
    "    # 3. Platform already has the question and answer stored\n",
    "    correct_solution = \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\"\n",
    "    # 4. Student provides attempt/solution\n",
    "    student_solution = input(\"Enter your solution: \")\n",
    "    # 5. Platform checks student's attempt\n",
    "    is_correct, feedback = check_solution(student_solution, correct_solution)\n",
    "    print(feedback)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question for students:\n",
      "Can you please let us know more details about your Write a function to calculate the factorial of a number.\n",
      "\n",
      "The factorial of a number is the product of all positive integers less than or equal to that number. For example, the factorial of 5 is 5*4*3*2*1 = 120.\n",
      "\n",
      "We need a function that takes an integer as input and returns the factorial of that number.\n",
      "\n",
      "Here is a Python code that calculates the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "Your solution is incorrect. Here's a hint: \n",
      "  if n == 0\n",
      "    1\n",
      "  else\n",
      "    n * factorial(n-1)\n",
      "  end\n",
      "end\n",
      "\n",
      "def factorial_iterative(n)\n",
      "  result = 1\n",
      "  while n > 0\n",
      "    result *= n\n",
      "    n -= 1\n",
      "  end\n",
      "  result\n",
      "end\n",
      "\n",
      "def factorial_recursive(n)\n",
      "  if n == 0\n",
      "    1\n",
      "  else\n",
      "    n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def generate_question(description):\n",
    "    # Generate question based on the description using the OpenChat API\n",
    "    output = query({\n",
    "        \"inputs\": \"Can you please let us know more details about your \" + description,\n",
    "    })\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = output[0]['generated_text']\n",
    "    return generated_text\n",
    "\n",
    "def check_solution(student_solution, correct_solution):\n",
    "    # Check if student's solution matches the correct solution\n",
    "    if student_solution == correct_solution:\n",
    "        return True, \"Congratulations! Your solution is correct.\"\n",
    "    else:\n",
    "        # Provide hint using OpenChat API\n",
    "        hint = generate_hint(student_solution)\n",
    "        return False, \"Your solution is incorrect. Here's a hint: \" + hint\n",
    "\n",
    "def generate_hint(student_solution):\n",
    "    # Generate hint based on the student's solution using the OpenChat API\n",
    "    # We'll simply append the student's solution with a request for a hint\n",
    "    hint_request = \"Can you provide a hint for improving my solution?\\n\\n\" + student_solution\n",
    "    hint_output = query({\"inputs\": hint_request})\n",
    "    hint = hint_output[0]['generated_text']\n",
    "    return hint\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # 1. Professor inputs description\n",
    "    professor_description = \"Write a function to calculate the factorial of a number.\"\n",
    "    # 2. Platform generates question\n",
    "    question = generate_question(professor_description)\n",
    "    print(\"Question for students:\")\n",
    "    print(question)\n",
    "    # 3. Platform already has the question and answer stored\n",
    "    correct_solution = \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\"\n",
    "    # 4. Student provides attempt/solution\n",
    "    student_solution = input(\"Enter your solution: \")\n",
    "    # 5. Platform checks student's attempt\n",
    "    is_correct, feedback = check_solution(student_solution, correct_solution)\n",
    "    print(feedback)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question for students:\n",
      "Can you please let us know more details about your Write a function to calculate the factorial of a number.\n",
      "\n",
      "The factorial of a number is the product of all positive integers less than or equal to that number. For example, the factorial of 5 is 5*4*3*2*1 = 120.\n",
      "\n",
      "We need a function that takes an integer as input and returns the factorial of that number.\n",
      "\n",
      "Here is a Python code that calculates the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "Your solution is incorrect. Here's a hint based on your response:\n",
      "Can you provide a hint for improving my solution?\n",
      "\n",
      "def factorial():\n",
      "    n = 1\n",
      "    for i in range(1, 6):\n",
      "        n *= i\n",
      "    return n\n",
      "\n",
      "def factorial_iterative(n):\n",
      "    result = 1\n",
      "    for i in range(1, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "def factorial_recursive(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n *\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def generate_question(description):\n",
    "    # Generate question based on the description using the OpenChat API\n",
    "    output = query({\n",
    "        \"inputs\": \"Can you please let us know more details about your \" + description,\n",
    "    })\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = output[0]['generated_text']\n",
    "    return generated_text\n",
    "\n",
    "def check_solution(student_solution, correct_solution):\n",
    "    # Check if student's solution matches the correct solution\n",
    "    if student_solution == correct_solution:\n",
    "        return True, \"Congratulations! Your solution is correct.\"\n",
    "    else:\n",
    "        # Provide tailored hint using OpenChat API\n",
    "        hint = generate_hint(student_solution)\n",
    "        return False, \"Your solution is incorrect. Here's a hint based on your response:\\n\" + hint\n",
    "\n",
    "def generate_hint(student_solution):\n",
    "    # Generate a tailored hint based on the student's solution using the OpenChat API\n",
    "    # We'll use the student's solution to ask for a hint\n",
    "    hint_request = \"Can you provide a hint for improving my solution?\\n\\n\" + student_solution\n",
    "    hint_output = query({\"inputs\": hint_request})\n",
    "    # Extract the hint from the response\n",
    "    hint = hint_output[0]['generated_text']\n",
    "    return hint\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # 1. Professor inputs description\n",
    "    professor_description = \"Write a function to calculate the factorial of a number.\"\n",
    "    # 2. Platform generates question\n",
    "    question = generate_question(professor_description)\n",
    "    print(\"Question for students:\")\n",
    "    print(question)\n",
    "    # 3. Platform already has the question and answer stored\n",
    "    correct_solution = \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\"\n",
    "    # 4. Student provides attempt/solution\n",
    "    student_solution = input(\"Enter your solution: \")\n",
    "    # 5. Platform checks student's attempt\n",
    "    is_correct, feedback = check_solution(student_solution, correct_solution)\n",
    "    print(feedback)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question for students:\n",
      "Can you please let us know more details about your Write a function to calculate the factorial of a number.\n",
      "\n",
      "The factorial of a number is the product of all positive integers less than or equal to that number. For example, the factorial of 5 is 5*4*3*2*1 = 120.\n",
      "\n",
      "We need a function that takes an integer as input and returns the factorial of that number.\n",
      "\n",
      "Here is a Python code that calculates the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "Your solution is incorrect. Here's a hint: def factorial():\n",
      "    n = int(input(\"Enter a number: \"))\n",
      "    if n < 0:\n",
      "        print(\"Sorry, factorial does not exist for negative numbers\")\n",
      "    elif n == 0:\n",
      "        print(\"The factorial of 0 is 1\")\n",
      "    else:\n",
      "        fact = 1\n",
      "        for i in range(1, n + 1):\n",
      "            fact = fact * i\n",
      "        print(\"The factorial of\", n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def generate_question(description):\n",
    "    # Generate question based on the description using the OpenChat API\n",
    "    output = query({\n",
    "        \"inputs\": \"Can you please let us know more details about your \" + description,\n",
    "    })\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = output[0]['generated_text']\n",
    "    return generated_text\n",
    "\n",
    "def check_solution(student_solution, correct_solution):\n",
    "    # Check if student's solution matches the correct solution\n",
    "    if student_solution == correct_solution:\n",
    "        return True, \"Congratulations! Your solution is correct.\"\n",
    "    else:\n",
    "        # Provide a tailored hint based on the student's solution\n",
    "        hint = generate_hint(student_solution)\n",
    "        return False, \"Your solution is incorrect. Here's a hint: \" + hint\n",
    "\n",
    "def generate_hint(student_solution):\n",
    "    # Generate a tailored hint based on the student's solution\n",
    "    # We'll use OpenChat API to generate a hint based on the student's response\n",
    "    hint_output = query({\"inputs\": student_solution})\n",
    "    hint = hint_output[0]['generated_text']\n",
    "    return hint\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # 1. Professor inputs description\n",
    "    professor_description = \"Write a function to calculate the factorial of a number.\"\n",
    "    # 2. Platform generates question\n",
    "    question = generate_question(professor_description)\n",
    "    print(\"Question for students:\")\n",
    "    print(question)\n",
    "    # 3. Platform already has the question and answer stored\n",
    "    correct_solution = \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\"\n",
    "    # 4. Student provides attempt/solution\n",
    "    student_solution = input(\"Enter your solution: \")\n",
    "    # 5. Platform checks student's attempt\n",
    "    is_correct, feedback = check_solution(student_solution, correct_solution)\n",
    "    print(feedback)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
